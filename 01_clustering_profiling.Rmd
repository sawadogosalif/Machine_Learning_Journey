---
title: "Clustering"
output:
  github_document:
    toc: true
    toc_depth: 2
---

# **Libraries**

```{r} 
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(dendextend) # for comparing two dendrograms--> Hierarchical Clustering Algorithms
library(Factoshiny) # automate analysis
library(FactoMineR) #unsupervised algorithm
library(plotly)     #dynamic plots
library(writexl)
```


# **My Functions**

```{r}
path="C:/Users/u32118508/OneDrive - UPEC/Bureau/Machine_learning_journey/A_journey_in_Machine_Learning/00_functions_clustering.R"
source(path)
```


# Environment
```{r}
setwd("C:/Users/u32118508/OneDrive - UPEC/Bureau/Machine_learning_journey/A_journey_in_Machine_Learning/INPUT")
data=dataset=read.table("student-por.csv",sep=";",header=TRUE)
head(data)
```
# **EDA**

## NA and data types
```{r}
data <- na.omit(data)
sapply(data, class)
```


Kmean is not applicable for categorial features
  
**1st solution** , run the algorithm with just continous fetaure

**2nd solution** ,transform  categorial features to continuous

## Transform categorial features to continuous features
<br />
List of non numeric columns.
```{r}
colnames(data)[grepl('factor|logical|character',sapply(data,class))]
```
<br />
Let's run MCA analysis.
```{r}
res.MCA<-MCA(data,quanti.sup=c(3,7,8,13,14,15,24,25,26,27,28,29,30,31,32,33),graph=FALSE,ncp=Inf)
res.MCA$eig
```
As one would expect the variables the factorial axes fail to summarize all the information in a very small number of axes. However, this is not our goal. We recover the coordinates of the individuals on the factorial axes.
<br />
We need column id fore future joint with numeric data
```{r}
cat=res.MCA$ind$coord
cat=as.data.frame(cat)
cat$Id  <- 1:nrow(cat)
head(round(cat,2))
```

## Scales continuous features
Continous features
```{r}
numeric_feature<-c("age","absences", "Medu", "Fedu", "freetime", "G1", 
                   "G2","G3" ,"goout" , "health" ,"studytime" ,"traveltime","Walc")
cont<-as.data.frame(scale(data[numeric_feature], center=FALSE, scale=FALSE))
cont$Id  <- 1:nrow(cont)
```

## Correlation  
```{r}
my_corr(cont)
```

## Final dataset for modeling
```{r}
cont$Id  <- 1:nrow(cont)
input_data<-cat %>%
            inner_join(cont,by='Id') %>%
            dplyr::select(-Id)
```

# **Clustering**

One challenge in clustering is tO determine the otimale number of cluster.
In practice, this step needs feedback between Data Scientist with Analyst and the Business/Operational teams.
Technically, we can propose the optimale number of cluster via some metrics( elbow, silhouette, Gap) but to ensure actionability of the clusters, we need to align with the Business Team.

Another challenge is that  in Kmeans and other clustering algorithm is is to reproduce  results. The Solutions to tackle this is :

• Increase number of iteration

• Seet Seed

• Initialize the algorithm with the  centers (for optimal output)


## Optimal k

Elbwo Curve 
```{r}
set.seed(10)
# Elbow method
fviz_nbclust(input_data, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")
```
K is between 3 an 7 d'après une lecture graphique. 
Mathematically, we have built a function which will allow us to choose the k with the right compromise between intra-class and inter-class inertia. On montre dans que le k optimale est 5

```{r}
 # calculating the within clusters sum-of-squares for 19 cluster amounts
   sum_of_squares = calculate_wcss(input_data)

# calculating the optimal number of clusters
  n = optimal_number_of_clusters(sum_of_squares)
  message("!!!!!optimal number of cluster: ", n)
```
Silhouette score

Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1. 1

**score = 1 **  : Means clusters are well apart from each other and clearly distinguished.

**score =-1** : Means clusters are assigned in the wrong way.


```{r}
set.seed(10)
# Silhouette method
fviz_nbclust(input_data, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```
In terms of improving the figure, working with 5 or 4 clusters is the same thing.
===> Let's do the analysis with 4 clusters.


## kmeans
```{r}
output <- kmeans(input_data, centers = 6, nstart = 25)
output$size
```
We have 4 clusters with the size above.
As it is known that the k mean is initialized random, no one can assure that the algorithm has converged. However, we have changed that the kmeans ++ version exists
```{r}
library(LICORS)
output2=kmeanspp(input_data, k = 6 , start = "random", iter.max = 100, nstart = 25)
output2$size
```
Kmean++ give the same output==> the algoritm converges.


```{r}
# Enhanced Kmean clustering
res.km <- eclust(x=input_data, 
                 FUN="kmeans",
                 hc_metric = "euclidean" ,
                 k=6)
#assess silhouette
fviz_silhouette(res.km) 
```
Individually, the silhouettes are not negative! good news.
```{r}
tmp=res.km$silinfo$widths
filter(tmp, sil_width<0)
```
```{r}
#variance explained
message ("variance explained :", res.km$betweenss/res.km$totss)
```

La séparations des classs explique àpeu près 60 % de la variabilité totale. Ce qui est synonyme de bon partitionnement.


## CAH


Dissimilarity matrix
```{r}
d = dist(input_data, method = "euclidean")
```
# Hierarchical clustering using Ward aggregation


Enhanced hierarchical clustering

```{r}
res.hc <- eclust(x=input_data, 
                 FUN="hclust",
                 hc_metric = "euclidean" ,
                 hc_method = "ward.D2",
                 k=6)
fviz_dend(res.hc, rect = TRUE) 
```
Silhouette score assessment
```{r}
fviz_silhouette(res.hc) 
```


Performance of the partitioning (silhouette) is bad, there are too many negative values.
worse Still: intra-class variance represents only 11% of the total inertia



```{r}
K <- n
ntotal<-dim(input_data)[1]
T <- sum(res.hc$height)
W <- sum(res.hc$height[1:(ntotal-K)])
# E
message ("variance explained :", (1-W/T))
```

# Descriptive statistics & profiling

Binaries features
```{r}

dataset$romantic <- ifelse(dataset$romantic == "yes",1,0)
dataset$internet <- ifelse(dataset$internet == "yes",1,0)
dataset$activities <- ifelse(dataset$activities == "yes",1,0)
dataset$schoolsup <- ifelse(dataset$schoolsup == "yes",1,0)
dataset$paid <- ifelse(dataset$paid == "yes",1,0)
dataset$sex<- ifelse(dataset$sex == "M",1,0)
binary_feature<-c("romantic", "internet", "sex","activities","paid","schoolsup")
```


Create dataset with the cluster number
```{r}
output<-dataset[c(binary_feature,numeric_feature)]
output<-data.frame(scale(output))
# create dataset with the cluster number
output$cluster <- (res.km$cluster)
```



 Reshape the data
```{r}
library(GGally)
temp <- gather(output, key="features", values,-cluster)
head(temp)
```

Compute clusers size
```{r}
size=output%>%
  group_by(cluster) %>%
  summarise(size=n())
```


Statistics : 

L'idée ici est de calculer les moyennes intra classes et la moyenne inter classe

```{r}
grp <- temp %>%
        group_by(cluster, features) %>%
         summarise(within_mean=mean(values),
                   sd=sd(values)) %>%
         ungroup() %>%
         data.frame()


grp=size%>%
   group_by(cluster) %>%
   right_join(grp, by='cluster') %>%
   mutate(w_mean= within_mean *  size / dim(input_data)[1]) %>%
   ungroup() %>%
   group_by(features) %>%
   mutate(between_mean= sum(w_mean)) %>%
  data.frame()

head(grp)
```



Clusters mapping

```{r}

grp$cluster_id=factor(grp$cluster)
ggplotly(ggplot(data=grp,aes(y=features, x=within_mean, fill=cluster_id))+
       facet_wrap(~cluster)+
       geom_bar(stat="identity") +
       geom_point(data=grp,aes(x=between_mean, y =features)))
```
This graph allows us to see how clusters are discriminated by features. 

**Cluster 1**:  Très Mauvais étudiants agés

Etudiants très agés ayant un nombre d'abscence elévé, mauvaises notes

**Cluster 2**:      Etudiants moyen 

Jeune étudiant  assidu  et profil moyen,  

**Cluster 3**:  Fils à maman 

Pas d'abscence, age au dessus de la moyenne, très faible notes +  maman scolarisée + voyages  - etudes  


**Cluster 4**:   Fille travailleuse

Sexe feminin + temps d'études et notes trés élévés                                                       (


**Cluster 5**:  Garçon étudiants mauvais

Garçons avec peu de temps d'études , abscense peu élévée et faibles notes.  


**Cluster 6**:   Abscent mais intelligent et passioné d' internet

beaucoup internet  + notes trés élévés +  abscense peu élévée + maman cadres


# Export Data with cluster label for supervised modeling


```{r}
output_df<-dataset[c(binary_feature,numeric_feature)]
output_df$cluster <- (res.km$cluster)
write_xlsx(output_df,"C:/Users/u32118508/OneDrive - UPEC/Bureau/Machine_learning_journey/A_journey_in_Machine_Learning/OUTPUT/output_tp1.xlsx")
```






